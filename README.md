# Petstore API Automation

Local test runner for TestSprite-generated API tests.

## ğŸ“‹ Overview

This project provides **local test execution** for TestSprite JSON-based API tests. Tests were generated by TestSprite AI from the [PRD document](docs/petstore-prd.md) and can be executed both in the cloud (TestSprite platform) and locally (using this runner).

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python Test    â”‚         â”‚   Proxy Server   â”‚
â”‚   Runner         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   (Node.js)      â”‚
â”‚   (Local)        â”‚  HTTP   â”‚   :3000          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ + API Key
                                      â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚  Petstore API    â”‚
                             â”‚  swagger.io/v2   â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** v18+ (for proxy server)
- **Python** 3.8+ (for test runner)
- **npm** (comes with Node.js)

### 1. Install Dependencies

```bash
# Install Node.js dependencies
npm install

# Install Python dependencies
pip install -r requirements.txt
```

### 2. Start the Proxy Server

The proxy server automatically adds API authentication headers:

```bash
npm start
```

You should see:
```
ğŸš€ Petstore API Proxy running on http://localhost:3000
ğŸ“¡ Proxying to: https://petstore.swagger.io/v2
ğŸ”‘ API Key: special-key
```

### 3. Run Tests Locally

In a **new terminal window**:

```bash
# Run all tests
python test_runner.py

# Or run specific test plan
python test_runner.py testsprite_tests/testsprite_backend_test_plan.json
```

## ğŸ“Š Test Results

The test runner provides:

- **âœ… Real-time test execution** with color-coded output
- **ğŸ“ˆ Pass/Fail statistics** and pass rate percentage
- **ğŸ” Detailed error messages** for failed tests
- **ğŸ“„ JSON test report** saved to `test_report.json`

### Sample Output

```
======================================================================
  TestSprite Local Test Runner
======================================================================

Project: petstore-api-automation
Test Type: backend
Base URL: http://localhost:3000
Test Plan: testsprite_tests/testsprite_backend_test_plan.json

[REQ-001] Pet Management - Create Pet
----------------------------------------------------------------------

  [PASS] [TC-001] Create pet with valid data
    POST http://localhost:3000/pet
    Status: 200 | Duration: 245.32ms

  [FAIL] [TC-002] Create pet without required name field
    POST http://localhost:3000/pet
    Status: 200 | Duration: 198.45ms
    Error: Status code mismatch: expected 405, got 200

...

======================================================================
  Test Summary
======================================================================

Total Tests:  32
Passed:       13
Failed:       19
Pass Rate:    40.6%
```

## ğŸ“ Project Structure

```
petstore-api-automation/
â”œâ”€â”€ testsprite_tests/              # TestSprite generated tests
â”‚   â”œâ”€â”€ testsprite_backend_test_plan.json  # 32 test cases
â”‚   â”œâ”€â”€ standard_prd.json          # Standardized PRD
â”‚   â””â”€â”€ tmp/                       # TestSprite metadata
â”œâ”€â”€ server/
â”‚   â””â”€â”€ proxy.js                   # Node.js proxy server
â”œâ”€â”€ config/
â”‚   â””â”€â”€ api-config.json            # API configuration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ petstore-prd.md            # Product Requirements Document
â”œâ”€â”€ test_runner.py                 # Local Python test runner â­
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ package.json                   # Node.js dependencies
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # This file
```

## ğŸ§ª Test Coverage

The test suite includes **32 test cases** covering:

### Pet Management (14 tests)
- âœ… Create, Read, Update, Delete pets
- âœ… Find pets by status
- âœ… Validation tests (required fields, invalid data)

### Store Operations (8 tests)
- âœ… Get inventory
- âœ… Place orders
- âœ… Retrieve and delete orders
- âœ… Order ID validation

### User Management (10 tests)
- âœ… Create, Read, Update, Delete users
- âœ… User login/logout
- âœ… User authentication

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# API Configuration
BASE_URL=https://petstore.swagger.io/v2
API_TIMEOUT=30

# TestSprite Configuration
TESTSPRITE_API_KEY=your_api_key_here
TESTSPRITE_PROJECT_ID=your_project_id_here

# Test Environment
TEST_ENV=staging
LOG_LEVEL=INFO
```

### API Configuration (config/api-config.json)

```json
{
  "apiName": "Petstore API",
  "baseUrl": "https://petstore.swagger.io/v2",
  "authentication": {
    "type": "apiKey",
    "headerName": "api_key",
    "value": "special-key"
  }
}
```

## ğŸ¯ Running Specific Tests

The test plan is organized by requirements. You can:

1. **Edit the test plan** to run specific tests only
2. **Filter by requirement ID** (edit JSON to include specific REQ-IDs)
3. **Modify test data** in the JSON file

## ğŸ“ Test Report

After execution, a detailed JSON report is saved to `test_report.json`:

```json
{
  "project_name": "petstore-api-automation",
  "test_type": "backend",
  "base_url": "http://localhost:3000",
  "timestamp": "2025-12-29T18:45:00.000000",
  "summary": {
    "total": 32,
    "passed": 13,
    "failed": 19,
    "pass_rate": 40.6
  },
  "results": [...]
}
```

## ğŸ” Troubleshooting

### Port 3000 Already in Use

```bash
# Kill existing process on port 3000
# Windows:
netstat -ano | findstr :3000
taskkill /PID <pid> /F

# Linux/Mac:
lsof -ti:3000 | xargs kill -9
```

### Connection Errors

Make sure the proxy server is running:
```bash
npm start
```

Test the proxy manually:
```bash
curl http://localhost:3000/store/inventory
```

### Python Module Not Found

```bash
pip install -r requirements.txt
```

## ğŸ†š TestSprite Cloud vs Local Execution

| Feature | TestSprite Cloud | Local Runner |
|---------|------------------|--------------|
| Test Generation | âœ… AI-powered | âŒ Uses existing tests |
| Execution | âœ… Cloud-based | âœ… Local machine |
| Reports | âœ… Cloud dashboard | âœ… JSON + Terminal |
| CI/CD Integration | âœ… API | âœ… Command-line |
| Cost | ğŸ’° Subscription | ğŸ†“ Free |
| Speed | âš¡ Parallel | ğŸ¢ Sequential |
| Debugging | âŒ Limited | âœ… Full control |

## ğŸ”— API Documentation

- **Petstore API**: https://petstore.swagger.io
- **Swagger Docs**: http://swagger.io
- **PRD Document**: [docs/petstore-prd.md](docs/petstore-prd.md)

## ğŸ“š Technologies Used

- **Python 3** - Test runner implementation
- **Node.js + Express** - Proxy server
- **TestSprite AI** - Test generation platform
- **Requests** - HTTP client library
- **JSON** - Test plan format

## ğŸ¤ Contributing

This project uses TestSprite-generated tests. To add new tests:

1. Update the PRD document
2. Regenerate tests using TestSprite
3. Or manually add test cases to `testsprite_backend_test_plan.json`

## ğŸ“„ License

See API documentation for terms of service.

## ğŸ› Known Issues

Some tests may fail due to:
- API behavior differences from PRD specification
- Missing test data (pre-created users, pets, orders)
- API validation rules that don't match expectations

These failures help identify discrepancies between the API specification and actual implementation.

---

**Made with** â¤ï¸ **using TestSprite AI Test Generation**
